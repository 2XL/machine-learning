{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing model acurracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas \n",
    "import matplotlib.pyplot\n",
    "import numpy\n",
    "\n",
    "# do ploting inline instead of in a separate window\n",
    "%matplotlib inline\n",
    "\n",
    "# \n",
    "df = pandas.read_csv(\"./data/pima-data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "70/30 - train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "feature_col_names = ['num_preg', 'glucose_conc', 'diastolic_bp', 'thickness', 'insulin','bmi', 'diab_pred', 'age']\n",
    "predicted_class_names = ['diabetes']\n",
    "\n",
    "X = df[feature_col_names].values # predictor feature columns (8 X m)\n",
    "Y = df[predicted_class_names].values # predicted class (1=True, 0=False) column (1 X m)\n",
    "split_test_size = 0.3\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=split_test_size, random_state=42)\n",
    "# test_size = 0.3 is 30% 42 is the answer to everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check to ensure we hav the desired 70% train, 30% test split of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set: \t 69.92%\ntesting set: \t 30.08%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "print \"training set: \\t {0:0.2f}%\".format(len(X_train)/len(df.index) * 100)\n",
    "print \"testing set: \\t {0:0.2f}%\".format(len(X_test)/len(df.index) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the splited data prediction values correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original True  : 268 (34.90%)\nOriginal False : 500 (65.10%)\n\nTraining True  : 188 (35.01%)\nTraining False : 349 (64.99%)\n\nTest True      : 80 (34.63%)\nTest False     : 151 (65.37%)\n"
     ]
    }
   ],
   "source": [
    "print \"Original True  : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 1]), (len(df.loc[df['diabetes'] == 1])/len(df.index)) * 100.0)\n",
    "print \"Original False : {0} ({1:0.2f}%)\".format(len(df.loc[df['diabetes'] == 0]), (len(df.loc[df['diabetes'] == 0])/len(df.index)) * 100.0)\n",
    "print \"\"\n",
    "print \"Training True  : {0} ({1:0.2f}%)\".format(len(Y_train[Y_train[:] == 1]), (len(Y_train[Y_train[:] == 1])/len(Y_train) * 100.0))\n",
    "print \"Training False : {0} ({1:0.2f}%)\".format(len(Y_train[Y_train[:] == 0]), (len(Y_train[Y_train[:] == 0])/len(Y_train) * 100.0))\n",
    "print \"\"\n",
    "print \"Test True      : {0} ({1:0.2f}%)\".format(len(Y_test[Y_test[:] == 1]), (len(Y_test[Y_test[:] == 1])/len(Y_test) * 100.0))\n",
    "print \"Test False     : {0} ({1:0.2f}%)\".format(len(Y_test[Y_test[:] == 0]), (len(Y_test[Y_test[:] == 0])/len(Y_test) * 100.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-split Data Preparation\n",
    "\n",
    "\n",
    "#### Hidding Missing Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# rows in dataframe 768\n# rows missing glucose_conc: 5\n# rows missing diastolic_bp: 35\n# rows missing thickness: 227\n# rows missing insulin: 374\n# rows missing bmi: 11\n# rows missing diab_pred: 0\n# rows missing age: 0\n"
     ]
    }
   ],
   "source": [
    "print \"# rows in dataframe {0}\".format(len(df))\n",
    "print \"# rows missing glucose_conc: {0}\".format(len(df.loc[df['glucose_conc'] == 0]))\n",
    "print \"# rows missing diastolic_bp: {0}\".format(len(df.loc[df['diastolic_bp'] == 0]))\n",
    "print \"# rows missing thickness: {0}\".format(len(df.loc[df['thickness'] == 0]))\n",
    "print \"# rows missing insulin: {0}\".format(len(df.loc[df['insulin'] == 0]))\n",
    "print \"# rows missing bmi: {0}\".format(len(df.loc[df['bmi'] == 0]))\n",
    "print \"# rows missing diab_pred: {0}\".format(len(df.loc[df['diab_pred'] == 0]))\n",
    "print \"# rows missing age: {0}\".format(len(df.loc[df['age'] == 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute with the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "\n",
    "# Impute with mean all 0 readings\n",
    "fill_0 = Imputer(missing_values=0, strategy=\"mean\")\n",
    "X_train = fill_0.fit(X_train)\n",
    "X_test = fill_0.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Initial Algorithm - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create Gaussian Naive Bayes model object and train it with the data\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(X_train, Y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values using the training data\n",
    "nb_predict_train = nb_model.predict(X_train)\n",
    "\n",
    "# Import the performance metrics library\n",
    "from sklearn import metrics\n",
    "\n",
    "# Accuracy\n",
    "print \"Accuracy: {0:.4f}\".format(metrics.accuracy_score(Y_train, nb_predict_train))\n",
    "print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7446\n"
     ]
    }
   ],
   "source": [
    "# Predict values using the testing data\n",
    "nb_predict_test = nb_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "print \"Accuracy: {0:.4f}\".format(metrics.accuracy_score(Y_test, nb_predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n[[119  32]\n [ 27  53]]\n\n[[True Negative] [False Positive]\n[False Negative] [True Positive]]\n\nClassification Report\n              precision    recall  f1-score   support\n\n       False       0.82      0.79      0.80       151\n        True       0.62      0.66      0.64        80\n\n   micro avg       0.74      0.74      0.74       231\n   macro avg       0.72      0.73      0.72       231\nweighted avg       0.75      0.74      0.75       231\n\n"
     ]
    }
   ],
   "source": [
    "print \"Confusion Matrix\"\n",
    "print \"{}\".format(metrics.confusion_matrix(Y_test, nb_predict_test))\n",
    "print \"\"\n",
    "print \"[[True Negative] [False Positive]\"\n",
    "print \"[False Negative] [True Positive]]\"\n",
    "print \"\"\n",
    "print \"Classification Report\"\n",
    "print metrics.classification_report(Y_test, nb_predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Improvement Options\n",
    "\n",
    "- Adjust current algorithm\n",
    "- Get more data or improve data\n",
    "- Improve training\n",
    "- Switch algorithms\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "- Ensemble Algorithm\n",
    "- Fits multiple trees with subsets of data\n",
    "- Averages tree results to improve performance and control overfitting\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Training data: 0.9888\nAccuracy Testing data: 0.7403\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=10)\n",
    "rf_model.fit(X_train, Y_train.ravel())\n",
    "\n",
    "# Predict Training Data\n",
    "rf_predict_train = rf_model.predict(X_train)\n",
    "# Training metrics\n",
    "print \"Accuracy Training data: {0:.4f}\".format(metrics.accuracy_score(Y_train, rf_predict_train))\n",
    "rf_predict_test = rf_model.predict(X_test)\n",
    "# Testing metrics\n",
    "print \"Accuracy Testing data: {0:.4f}\".format(metrics.accuracy_score(Y_test, rf_predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123  28]\n [ 32  48]]\n\nClassification Report\n              precision    recall  f1-score   support\n\n       False       0.79      0.81      0.80       151\n        True       0.63      0.60      0.62        80\n\n   micro avg       0.74      0.74      0.74       231\n   macro avg       0.71      0.71      0.71       231\nweighted avg       0.74      0.74      0.74       231\n\n"
     ]
    }
   ],
   "source": [
    "print metrics.confusion_matrix(Y_test, rf_predict_test)\n",
    "print \"\"\n",
    "print \"Classification Report\"\n",
    "print metrics.classification_report(Y_test, rf_predict_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
